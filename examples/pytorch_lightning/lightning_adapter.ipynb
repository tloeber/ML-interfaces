{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note on license:\n",
    "This notebook uses examples from the official Lightning repo, which is licensed under Apache 2.0. In compliance with the Apache license, any reused code is relicensed under the license in this project (as of September 2023, the MIT license), but I list modifications to the original code here:\n",
    "- Refactor PyTorch Lightning example code so it can be used with an adapter class from this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't use strict note for type checking yet\n",
    "%nb_mypy mypy-options --pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod, abstractproperty, ABC\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Interface\n",
    "# =========\n",
    "\n",
    "class DataSetInterface(ABC):\n",
    "    \"\"\"\n",
    "    This serves as the *abstract* type under which all the concrete dataset \n",
    "    interfaces fall. We can use when we want to depend only on the data set \n",
    "    abstraction, but not the concrete type of data set. \n",
    "    \n",
    "    At the moment, this interface does not yet defined any shared behavior,\n",
    "    so it would also be possible to use virtual subclasses (e.g., registering) \n",
    "    instead. However, we want to keep the option open for the future to define \n",
    "    shared behavior that all the concrete dataset interfaces must implement.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# Implementation\n",
    "# ==============\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "\n",
    "class ImageDataSetImplementation(DataSetInterface):\n",
    "    def __init__(self, data: TorchDataset):\n",
    "        self.data = data\n",
    "    \n",
    "    @classmethod\n",
    "    def from_torch(cls, data):\n",
    "        return cls(data=data)\n",
    "\n",
    "    def to_torch(self) -> TorchDataset:\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Container\n",
    "# =========\n",
    "\n",
    "class DataContainerInterface(ABC):\n",
    "    @abstractproperty\n",
    "    def train(self) -> DataSetInterface:\n",
    "        pass\n",
    "\n",
    "    @abstractproperty\n",
    "    def val(self) -> DataSetInterface:\n",
    "        pass\n",
    "\n",
    "    @abstractproperty\n",
    "    def test(self) -> DataSetInterface:\n",
    "        pass\n",
    "\n",
    "\n",
    "class DataContainer():\n",
    "    \"\"\"\n",
    "    This class is using the simplest possible way to define a container: It \n",
    "    simply has a property for each of the three subsets. Note that we are not \n",
    "    implementing generics yet to distinguish containers for different kinds of \n",
    "    data sets.\n",
    "    (Note also that it does not have a method implemented to retrieve the \n",
    "    complete set, because this is not necessary for our current testing \n",
    "    purposes.) \n",
    "    \"\"\"\n",
    "    def __init__(self, train: DataSetInterface, val: DataSetInterface, test: DataSetInterface):\n",
    "        self._train = train\n",
    "        self._val = val\n",
    "        self._test = test\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        return self._train\n",
    "    \n",
    "    @property\n",
    "    def val(self):\n",
    "        return self._val\n",
    "    \n",
    "    @property\n",
    "    def test(self):\n",
    "        return self._test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from lightning.pytorch import LightningDataModule, LightningModule\n",
    "\n",
    "class TorchDataloaderAdapter(LightningDataModule):\n",
    "    \"\"\"\n",
    "    This adapter class takes an instance of our data container and converts it \n",
    "    into a LightningDataModule.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_container, batch_size) -> None:\n",
    "        super().__init__()\n",
    "        self.data_container = data_container\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.data_container.train.to_torch(),\n",
    "            batch_size=self.batch_size\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.data_container.val.to_torch(), \n",
    "            batch_size=self.batch_size\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.data_container.test.to_torch(), \n",
    "            batch_size=self.batch_size\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self, data):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example data\n",
    "# ===================\n",
    " \n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "DATA_DIR = './data'\n",
    "\n",
    "mnist_train_and_val = MNIST(\n",
    "    root=DATA_DIR, \n",
    "    train=True, \n",
    "    download=True,\n",
    "        transform=transforms.ToTensor()\n",
    ")\n",
    "mnist_train, mnist_val = random_split(\n",
    "    dataset=mnist_train_and_val, \n",
    "    lengths=[.9, .1]\n",
    ")\n",
    "mnist_test = MNIST(\n",
    "    root=DATA_DIR, \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "mnist_train = ImageDataSetImplementation.from_torch(mnist_train)\n",
    "mnist_val = ImageDataSetImplementation.from_torch(mnist_val)\n",
    "mnist_test = ImageDataSetImplementation.from_torch(mnist_test)\n",
    "\n",
    "mnist_container = DataContainer(\n",
    "    train=mnist_train,\n",
    "    val=mnist_val,\n",
    "    test=mnist_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/thomas/.cache/pypoetry/virtualenvs/oject-oriented-ml-2RS15okd-py3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n",
      "\n",
      "  | Name     | Type     | Params\n",
      "--------------------------------------\n",
      "0 | backbone | Backbone | 101 K \n",
      "--------------------------------------\n",
      "101 K     Trainable params\n",
      "0         Non-trainable params\n",
      "101 K     Total params\n",
      "0.407     Total estimated model params size (MB)\n",
      "/home/thomas/.cache/pypoetry/virtualenvs/oject-oriented-ml-2RS15okd-py3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/thomas/.cache/pypoetry/virtualenvs/oject-oriented-ml-2RS15okd-py3.11/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "/home/thomas/.cache/pypoetry/virtualenvs/oject-oriented-ml-2RS15okd-py3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eb94f51837b404f89039dc751856614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19370aeea6b14825ad75e5869b00854e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-13 09:07:43.412314\n"
     ]
    }
   ],
   "source": [
    "# EstimatorInterface\n",
    "# ==================\n",
    "\n",
    "class EstimatorInterface(ABC):\n",
    "    @abstractmethod\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    # @abstractmethod\n",
    "    # def optimize_hyperparameters(self):\n",
    "    #     pass\n",
    "\n",
    "    # @abstractmethod\n",
    "    # def predict(self) -> DataSetInterface:\n",
    "    #     pass\n",
    "\n",
    "    # @abstractmethod\n",
    "    # def evaluate(self):\n",
    "    #     pass\n",
    "\n",
    "    # @abstractmethod\n",
    "    # def main(self):\n",
    "    #     pass\n",
    "\n",
    "\n",
    "# EstimatorImplementation\n",
    "# =======================\n",
    "\n",
    "from datetime import datetime \n",
    "from pydantic import BaseModel\n",
    "import lightning as L\n",
    "\n",
    "from utils.lightning_adapter import LitClassifier\n",
    "\n",
    "\n",
    "class ModelConfig(BaseModel):\n",
    "    batch_size: int\n",
    "    fast_dev_run: bool = False\n",
    "    \n",
    "\n",
    "class PytorchLightningAdapter(EstimatorInterface):\n",
    "    \"\"\"\n",
    "    This adaptor class takes a PyTorch Lightning classifier and converts its \n",
    "    interface to our OO_ML estimator interface.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        Classifier: type[LightningModule],\n",
    "        data_container: DataContainer,\n",
    "        config: ModelConfig,\n",
    "    ) -> None:\n",
    "        self.config = config\n",
    "        self.classifier = Classifier()\n",
    "        # Convert data container to lightning data loader\n",
    "        self.data_loader =  TorchDataloaderAdapter(\n",
    "            data_container=data_container,\n",
    "            batch_size=config.batch_size,    \n",
    "        )\n",
    "        \n",
    "    \n",
    "    def fit(self):\n",
    "        # Move trainer instantiation to __init__() if it needs to be accessed outside this method.\n",
    "        trainer = L.Trainer(fast_dev_run=config.fast_dev_run)\n",
    "        trainer.fit(\n",
    "            model=self.classifier,\n",
    "            datamodule=self.data_loader,\n",
    "        )\n",
    "\n",
    "    # def optimize_hyperparameters(self):\n",
    "    # def predict(self) -> DataSetInterface:\n",
    "    # def evaluate(self):\n",
    "    # def main(self):\n",
    "        # self.optimize_hyperparameters()\n",
    "        # self.evaluate()\n",
    "        # self.predict(data: DataInputInterface) -> DataOutputInterface:\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = ModelConfig(\n",
    "        batch_size=64,\n",
    "        fast_dev_run=True,\n",
    "    )\n",
    "\n",
    "    model = PytorchLightningAdapter(\n",
    "        Classifier=LitClassifier,\n",
    "        data_container=mnist_container,\n",
    "        config=config,\n",
    "    )\n",
    "    model.fit()\n",
    "\n",
    "    print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oject-oriented-ml-2RS15okd-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
