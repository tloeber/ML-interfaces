{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note on license:\n",
    "This notebook uses examples from the official Lightning repo, which is licensed under Apache 2.0. In compliance with the Apache license, any reused code is relicensed under the license in this project (as of September 2023, the MIT license), but I list modifications to the original code here:\n",
    "- Refactor PyTorch Lightning example code so it can be used with an adapter class from this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't use strict note for type checking yet\n",
    "%nb_mypy mypy-options --pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<cell>63: \u001b[34mnote:\u001b[m \u001b[m\u001b[1m\"StructuredDataSetImplementation\"\u001b[m defined here\u001b[m\n",
      "<cell>77: \u001b[1m\u001b[31merror:\u001b[m Unexpected keyword argument \u001b[m\u001b[1m\"pd_data_frame\"\u001b[m for\n",
      "<cell>77: \u001b[1m\u001b[31merror:\u001b[m Unexpected keyword argument \u001b[m\u001b[1m\"target_name\"\u001b[m for\n",
      "<cell>80: \u001b[1m\u001b[31merror:\u001b[m \u001b[m\u001b[1m\"StructuredDataSetImplementation\"\u001b[m has no attribute \u001b[m\u001b[1m\"data\"\u001b[m \n",
      "<cell>85: \u001b[1m\u001b[31merror:\u001b[m Unexpected keyword argument \u001b[m\u001b[1m\"pd_data_frame\"\u001b[m for\n",
      "<cell>88: \u001b[1m\u001b[31merror:\u001b[m \u001b[m\u001b[1m\"StructuredDataSetImplementation\"\u001b[m has no attribute \u001b[m\u001b[1m\"data\"\u001b[m \n",
      "<cell>91: \u001b[1m\u001b[31merror:\u001b[m \u001b[m\u001b[1m\"StructuredDataSetImplementation\"\u001b[m has no attribute\n"
     ]
    }
   ],
   "source": [
    "from abc import abstractmethod, abstractproperty, ABC\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Interface\n",
    "# =========\n",
    "\n",
    "class DataSetInterface(ABC):\n",
    "    \"\"\"\n",
    "    This serves as the *abstract* type under which all the concrete dataset \n",
    "    interfaces fall. We can use when we want to depend only on the data set \n",
    "    abstraction, but not the concrete type of data set. \n",
    "    \n",
    "    At the moment, this interface does not yet defined any shared behavior,\n",
    "    so it would also be possible to use virtual subclasses (e.g., registering) \n",
    "    instead. However, we want to keep the option open for the future to define \n",
    "    shared behavior that all the concrete dataset interfaces must implement.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "class StructuredDataSetInterface(DataSetInterface):\n",
    "    \n",
    "    @abstractproperty\n",
    "    def X(self):\n",
    "        pass\n",
    "\n",
    "    @abstractproperty\n",
    "    def y(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    # Format conversions\n",
    "    # ------------------\n",
    "    @classmethod\n",
    "    @abstractmethod\n",
    "    def from_pandas(cls, input_data: pd.DataFrame):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def to_pandas(self) -> pd.DataFrame:\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    @abstractmethod\n",
    "    def from_numpy(cls, input_data: np.ndarray):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def to_numpy(self) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_column_names(self) -> list[str]:\n",
    "        pass\n",
    "\n",
    "\n",
    "# Implementation\n",
    "# ==============\n",
    "\n",
    "class StructuredDataSetImplementation(DataSetInterface):\n",
    "    def __init__(self, X: pd.DataFrame, y: pd.DataFrame):\n",
    "        self._X = X\n",
    "        self._y = y\n",
    "\n",
    "    @property\n",
    "    def X(self):\n",
    "        return self._X\n",
    "    \n",
    "    @property\n",
    "    def y(self):\n",
    "        return self._y\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pandas(cls, input_data: pd.DataFrame, target_name: str):\n",
    "        return cls(pd_data_frame=input_data, target_name=target_name)\n",
    "\n",
    "    def to_pandas(self) -> pd.DataFrame:\n",
    "        return self.data    \n",
    "            \n",
    "    @classmethod\n",
    "    def from_numpy(cls, input_data: np.ndarray):\n",
    "        pd_data_frame=pd.DataFrame(input_data)\n",
    "        return cls(pd_data_frame=pd_data_frame)\n",
    "\n",
    "    def to_numpy(self) -> np.ndarray:\n",
    "        return self.data.to_numpy()\n",
    "    \n",
    "    def get_column_names(self) -> list[str]:\n",
    "        return self.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Container\n",
    "# =========\n",
    "\n",
    "class DataContainerInterface(ABC):\n",
    "    @abstractproperty\n",
    "    def train(self) -> DataSetInterface:\n",
    "        pass\n",
    "\n",
    "    @abstractproperty\n",
    "    def val(self) -> DataSetInterface:\n",
    "        pass\n",
    "\n",
    "    @abstractproperty\n",
    "    def test(self) -> DataSetInterface:\n",
    "        pass\n",
    "\n",
    "\n",
    "class DataContainer():\n",
    "    def __init__(self, train: DataSetInterface, val: DataSetInterface, test: DataSetInterface):\n",
    "        self._train = train\n",
    "        self._val = val\n",
    "        self._test = test\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        return self._train\n",
    "    \n",
    "    @property\n",
    "    def val(self):\n",
    "        return self._val\n",
    "    \n",
    "    @property\n",
    "    def test(self):\n",
    "        return self._test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from lightning.pytorch import LightningDataModule, LightningModule\n",
    "\n",
    "class TorchDataloaderAdapter(LightningDataModule):\n",
    "    def __init__(self, data_container, batch_size) -> None:\n",
    "        super().__init__()\n",
    "        self.data_container = data_container\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.data_container.train.to_torch(),\n",
    "            batch_size=self.batch_size\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.data_container.val.to_torch(), \n",
    "            batch_size=self.batch_size\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.data_container.test.to_torch(), \n",
    "            batch_size=self.batch_size\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self, data):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset as TorchDataset\n",
    "\n",
    "class ImageDataSetImplementation(DataSetInterface):\n",
    "    def __init__(self, data: TorchDataset):\n",
    "        self.data = data\n",
    "    \n",
    "    @classmethod\n",
    "    def from_torch(cls, data):\n",
    "        return cls(data=data)\n",
    "\n",
    "    def to_torch(self) -> TorchDataset:\n",
    "        return self.data\n",
    "    \n",
    "    def to_torch_dataloader(self, batch_size) -> LightningDataModule:\n",
    "        return TorchDataloaderAdapter(\n",
    "            data_container=self, \n",
    "            batch_size=batch_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example data\n",
    "# ===================\n",
    " \n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "DATA_DIR = './data'\n",
    "\n",
    "mnist_train_and_val = MNIST(\n",
    "    root=DATA_DIR, \n",
    "    train=True, \n",
    "    download=True,\n",
    "        transform=transforms.ToTensor()\n",
    ")\n",
    "mnist_train, mnist_val = random_split(\n",
    "    dataset=mnist_train_and_val, \n",
    "    lengths=[.9, .1]\n",
    ")\n",
    "mnist_test = MNIST(\n",
    "    root=DATA_DIR, \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "mnist_train = ImageDataSetImplementation.from_torch(mnist_train)\n",
    "mnist_val = ImageDataSetImplementation.from_torch(mnist_val)\n",
    "mnist_test = ImageDataSetImplementation.from_torch(mnist_test)\n",
    "\n",
    "mnist_container = DataContainer(\n",
    "    train=mnist_train,\n",
    "    val=mnist_val,\n",
    "    test=mnist_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/thomas/.cache/pypoetry/virtualenvs/oject-oriented-ml-2RS15okd-py3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n",
      "\n",
      "  | Name     | Type     | Params\n",
      "--------------------------------------\n",
      "0 | backbone | Backbone | 101 K \n",
      "--------------------------------------\n",
      "101 K     Trainable params\n",
      "0         Non-trainable params\n",
      "101 K     Total params\n",
      "0.407     Total estimated model params size (MB)\n",
      "/home/thomas/.cache/pypoetry/virtualenvs/oject-oriented-ml-2RS15okd-py3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/thomas/.cache/pypoetry/virtualenvs/oject-oriented-ml-2RS15okd-py3.11/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "/home/thomas/.cache/pypoetry/virtualenvs/oject-oriented-ml-2RS15okd-py3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2e0439d5fbf4d6fbab86e72323fa53b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383da1ca84aa46dab7c3356f6072052f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime \n",
    "from pydantic import BaseModel\n",
    "import lightning as L\n",
    "\n",
    "from lightning_adapter_extras import LitClassifier\n",
    "\n",
    "\n",
    "class ModelConfig(BaseModel):\n",
    "    batch_size: int\n",
    "    fast_dev_run: bool = False\n",
    "    \n",
    "\n",
    "class PytorchLightningAdapter():\n",
    "    def __init__(\n",
    "        self, \n",
    "        Classifier: type[LightningModule],\n",
    "        data_container: DataContainer,\n",
    "        config: ModelConfig,\n",
    "    ) -> None:\n",
    "        self.config = config\n",
    "        self.classifier = Classifier()\n",
    "        \n",
    "        self.data_loader =  TorchDataloaderAdapter(\n",
    "            data_container=data_container,\n",
    "            batch_size=config.batch_size,    \n",
    "        )\n",
    "        \n",
    "    \n",
    "    def fit(self):\n",
    "        # Move trainer instantiation to __init__() if it needs to be accessed outside this method.\n",
    "        trainer = L.Trainer(fast_dev_run=config.fast_dev_run)\n",
    "        trainer.fit(\n",
    "            model=self.classifier,\n",
    "            datamodule=self.data_loader,\n",
    "        )\n",
    "\n",
    "    # def optimize_hyperparameters(self):\n",
    "    # def predict(self) -> DataSetInterface:\n",
    "    # def evaluate(self):\n",
    "    # def main(self):\n",
    "        # self.optimize_hyperparameters()\n",
    "        # self.evaluate()\n",
    "        # self.predict(data: DataInputInterface) -> DataOutputInterface:\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = ModelConfig(\n",
    "        batch_size=64,\n",
    "        fast_dev_run=True,\n",
    "    )\n",
    "\n",
    "    model = PytorchLightningAdapter(\n",
    "        Classifier=LitClassifier,\n",
    "        data_container=mnist_container,\n",
    "        config=config,\n",
    "    )\n",
    "    model.fit()\n",
    "\n",
    "    # print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oject-oriented-ml-2RS15okd-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
