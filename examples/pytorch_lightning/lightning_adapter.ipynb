{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note on license:\n",
    "This notebook uses examples from the official Lightning repo, which is licensed under Apache 2.0. In compliance with the Apache license, any reused code is relicensed under the license in this project (as of September 2023, the MIT license), but I list modifications to the original code here:\n",
    "- Refactor PyTorch Lightning example code so it can be used with an adapter class from this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't use strict note for type checking yet\n",
    "%nb_mypy mypy-options --pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAST_DEV_RUN=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<cell>63: \u001b[34mnote:\u001b[m \u001b[m\u001b[1m\"StructuredDataSetImplementation\"\u001b[m defined here\u001b[m\n",
      "<cell>77: \u001b[1m\u001b[31merror:\u001b[m Unexpected keyword argument \u001b[m\u001b[1m\"pd_data_frame\"\u001b[m for\n",
      "<cell>77: \u001b[1m\u001b[31merror:\u001b[m Unexpected keyword argument \u001b[m\u001b[1m\"target_name\"\u001b[m for\n",
      "<cell>80: \u001b[1m\u001b[31merror:\u001b[m \u001b[m\u001b[1m\"StructuredDataSetImplementation\"\u001b[m has no attribute \u001b[m\u001b[1m\"data\"\u001b[m \n",
      "<cell>85: \u001b[1m\u001b[31merror:\u001b[m Unexpected keyword argument \u001b[m\u001b[1m\"pd_data_frame\"\u001b[m for\n",
      "<cell>88: \u001b[1m\u001b[31merror:\u001b[m \u001b[m\u001b[1m\"StructuredDataSetImplementation\"\u001b[m has no attribute \u001b[m\u001b[1m\"data\"\u001b[m \n",
      "<cell>91: \u001b[1m\u001b[31merror:\u001b[m \u001b[m\u001b[1m\"StructuredDataSetImplementation\"\u001b[m has no attribute\n"
     ]
    }
   ],
   "source": [
    "from abc import abstractmethod, abstractproperty, ABC\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Interface\n",
    "# =========\n",
    "\n",
    "class DataSetInterface(ABC):\n",
    "    \"\"\"\n",
    "    This serves as the *abstract* type under which all the concrete dataset \n",
    "    interfaces fall. We can use when we want to depend only on the data set \n",
    "    abstraction, but not the concrete type of data set. \n",
    "    \n",
    "    At the moment, this interface does not yet defined any shared behavior,\n",
    "    so it would also be possible to use virtual subclasses (e.g., registering) \n",
    "    instead. However, we want to keep the option open for the future to define \n",
    "    shared behavior that all the concrete dataset interfaces must implement.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "class StructuredDataSetInterface(DataSetInterface):\n",
    "    \n",
    "    @abstractproperty\n",
    "    def X(self):\n",
    "        pass\n",
    "\n",
    "    @abstractproperty\n",
    "    def y(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    # Format conversions\n",
    "    # ------------------\n",
    "    @classmethod\n",
    "    @abstractmethod\n",
    "    def from_pandas(cls, input_data: pd.DataFrame):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def to_pandas(self) -> pd.DataFrame:\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    @abstractmethod\n",
    "    def from_numpy(cls, input_data: np.ndarray):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def to_numpy(self) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_column_names(self) -> list[str]:\n",
    "        pass\n",
    "\n",
    "\n",
    "# Implementation\n",
    "# ==============\n",
    "\n",
    "class StructuredDataSetImplementation(DataSetInterface):\n",
    "    def __init__(self, X: pd.DataFrame, y: pd.DataFrame):\n",
    "        self._X = X\n",
    "        self._y = y\n",
    "\n",
    "    @property\n",
    "    def X(self):\n",
    "        return self._X\n",
    "    \n",
    "    @property\n",
    "    def y(self):\n",
    "        return self._y\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pandas(cls, input_data: pd.DataFrame, target_name: str):\n",
    "        return cls(pd_data_frame=input_data, target_name=target_name)\n",
    "\n",
    "    def to_pandas(self) -> pd.DataFrame:\n",
    "        return self.data    \n",
    "            \n",
    "    @classmethod\n",
    "    def from_numpy(cls, input_data: np.ndarray):\n",
    "        pd_data_frame=pd.DataFrame(input_data)\n",
    "        return cls(pd_data_frame=pd_data_frame)\n",
    "\n",
    "    def to_numpy(self) -> np.ndarray:\n",
    "        return self.data.to_numpy()\n",
    "    \n",
    "    def get_column_names(self) -> list[str]:\n",
    "        return self.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Container\n",
    "# =========\n",
    "\n",
    "class DataContainerInterface(ABC):\n",
    "    @abstractproperty\n",
    "    def train(self) -> DataSetInterface:\n",
    "        pass\n",
    "\n",
    "    @abstractproperty\n",
    "    def val(self) -> DataSetInterface:\n",
    "        pass\n",
    "\n",
    "    @abstractproperty\n",
    "    def test(self) -> DataSetInterface:\n",
    "        pass\n",
    "\n",
    "\n",
    "class DataContainer():\n",
    "    def __init__(self, train: DataSetInterface, val: DataSetInterface, test: DataSetInterface):\n",
    "        self._train = train\n",
    "        self._val = val\n",
    "        self._test = test\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        return self._train\n",
    "    \n",
    "    @property\n",
    "    def val(self):\n",
    "        return self._val\n",
    "    \n",
    "    @property\n",
    "    def test(self):\n",
    "        return self._test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset as TorchDataset\n",
    "\n",
    "class ImageDataSetImplementation(DataSetInterface):\n",
    "    def __init__(self, data: TorchDataset):\n",
    "        self.data = data\n",
    "    \n",
    "    @classmethod\n",
    "    def from_torch(cls, data):\n",
    "        return cls(data=data)\n",
    "\n",
    "    def to_torch(self) -> TorchDataset:\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 16784757.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 11823884.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 12688009.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 25709215.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create example data\n",
    "# ===================\n",
    " \n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "from lightning.pytorch import LightningDataModule, LightningModule\n",
    "from lightning.pytorch.utilities.imports import _TORCHVISION_AVAILABLE\n",
    "\n",
    "if _TORCHVISION_AVAILABLE:\n",
    "    from torchvision import transforms\n",
    "\n",
    "\n",
    "DATA_DIR = './data'\n",
    "\n",
    "mnist_train_and_val = MNIST(\n",
    "    root=DATA_DIR, \n",
    "    train=True, \n",
    "    download=True,\n",
    "        transform=transforms.ToTensor()\n",
    ")\n",
    "mnist_train, mnist_val = random_split(\n",
    "    dataset=mnist_train_and_val, \n",
    "    lengths=[.9, .1]\n",
    ")\n",
    "mnist_test = MNIST(\n",
    "    root=DATA_DIR, \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "mnist_train = ImageDataSetImplementation.from_torch(mnist_train)\n",
    "mnist_val = ImageDataSetImplementation.from_torch(mnist_val)\n",
    "mnist_test = ImageDataSetImplementation.from_torch(mnist_test)\n",
    "\n",
    "mnist_container = DataContainer(\n",
    "    train=mnist_train,\n",
    "    val=mnist_val,\n",
    "    test=mnist_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchDataloaderAdapter(LightningDataModule):\n",
    "    def __init__(self, data_container, batch_size) -> None:\n",
    "        super().__init__()\n",
    "        self.data_container = data_container\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.data_container.train.to_torch(),\n",
    "            batch_size=self.batch_size\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.data_container.val.to_torch(), \n",
    "            batch_size=self.batch_size\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.data_container.test.to_torch(), \n",
    "            batch_size=self.batch_size\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self, data):\n",
    "        pass\n",
    "\n",
    "my_dataloader = TorchDataloaderAdapter(\n",
    "    data_container=mnist_container,\n",
    "    batch_size=64,    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<cell>4: \u001b[1m\u001b[31merror:\u001b[m Relative import climbs too many namespaces  \u001b[m\u001b[33m[misc]\u001b[m\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/thomas/repos/object-oriented-ML/examples/pytorch_lightning/lightning_adapter.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/thomas/repos/object-oriented-ML/examples/pytorch_lightning/lightning_adapter.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpydantic\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseModel\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/thomas/repos/object-oriented-ML/examples/pytorch_lightning/lightning_adapter.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlightning\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mL\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/thomas/repos/object-oriented-ML/examples/pytorch_lightning/lightning_adapter.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlightning_adapter_extras\u001b[39;00m \u001b[39mimport\u001b[39;00m LitClassifier\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/thomas/repos/object-oriented-ML/examples/pytorch_lightning/lightning_adapter.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Inner config objects\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/thomas/repos/object-oriented-ML/examples/pytorch_lightning/lightning_adapter.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mSaveConfigKwargs\u001b[39;00m(BaseModel): \n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "import lightning as L\n",
    "\n",
    "from ..lightning_adapter_extras import LitClassifier\n",
    "\n",
    "\n",
    "# Inner config objects\n",
    "class SaveConfigKwargs(BaseModel): \n",
    "    overwrite: bool\n",
    "    \n",
    "\n",
    "# Main config\n",
    "class Config(BaseModel):\n",
    "    batch_size: int\n",
    "    # learning_rate: float = 0.001\n",
    "    # hidden_dim: int = 128\n",
    "    # data_dir: str = \"./data\"\n",
    "    run: bool\n",
    "    save_config_kwargs: SaveConfigKwargs\n",
    "    seed_everything_default: int = 1\n",
    "        \n",
    "\n",
    "class PytorchLightningAdapter():\n",
    "    def __init__(\n",
    "        self, \n",
    "        classifier: LightningModule,\n",
    "        data_container: DataContainer,\n",
    "        config: Config,\n",
    "    ) -> None:\n",
    "        self.data_container = data_container\n",
    "        self.classifier = classifier\n",
    "        self.config = config\n",
    "\n",
    "    # def optimize_hyperparameters(self):\n",
    "    # def fit(self) -> None:\n",
    "    # def predict(self) -> DataSetInterface:\n",
    "\n",
    "\n",
    "    # def main(self):\n",
    "        \n",
    "    #     cli = LightningCLI(\n",
    "    #         model_class=self.classifier,\n",
    "    #         datamodule_class= create_torch_dataloader(\n",
    "    #             self.data_container, \n",
    "    #             batch_size=config.batch_size\n",
    "    #         ),\n",
    "    #         seed_everything_default=self.config.seed_everything_default,\n",
    "    #         save_config_kwargs=self.config.save_config_kwargs.dict(),\n",
    "    #         run=self.config.run,\n",
    "    #     )\n",
    "    #     cli.trainer.fit(cli.model, datamodule=cli.datamodule)\n",
    "    #     cli.trainer.test(ckpt_path=\"best\", datamodule=cli.datamodule)\n",
    "    #     predictions = cli.trainer.predict(ckpt_path=\"best\", datamodule=cli.datamodule)\n",
    "    #     if predictions is not None:\n",
    "    #         print(predictions[0])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = Config(\n",
    "        batch_size=64,\n",
    "        save_config_kwargs=SaveConfigKwargs(overwrite=True),\n",
    "        run=False,\n",
    "    )\n",
    "    \n",
    "    classifier=LitClassifier()\n",
    "    trainer = L.Trainer(fast_dev_run=FAST_DEV_RUN)\n",
    "    \n",
    "    trainer.fit(\n",
    "        model=classifier,\n",
    "        # data.DataLoader(train), data.DataLoader(val))\n",
    "        datamodule=my_dataloader,\n",
    "    )\n",
    "    # ptl = PytorchLightningAdapter(\n",
    "    #     classifier=LitClassifier,\n",
    "    #     data_container=mnist_container,\n",
    "    #     config=config,\n",
    "    # ptl.cli_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oject-oriented-ml-2RS15okd-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
