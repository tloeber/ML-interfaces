{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (497133051.py, line 115)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 115\u001b[0;36m\u001b[0m\n\u001b[0;31m    def __init__(self, dataset: TorchDatasetProtocol: TypeAlias = TorchMapDatasetProtocol | TorchIterableDatasetType: TypeAlias = TorchMapDatasetProtocol | TorchIterableDatasetType: TypeAlias = TorchMapDatasetProtocol | TorchIterableDatasetType: TypeAlias = TorchMapDatasetProtocol | TorchIterableDatasetType, gpu: bool = False) -> None:\u001b[0m\n\u001b[0m                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from abc import abstractmethod, abstractproperty, ABC\n",
    "from typing import Callable, Iterator, TypeAlias, NoReturn, Any\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Interface\n",
    "# =========\n",
    "\n",
    "\n",
    "from abc import abstractmethod, ABC\n",
    "from typing import Callable, Iterator, Any, Protocol\n",
    "\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "\n",
    "from data_protocols import TorchMapDatasetProtocol, TorchIterableDatasetProtocol, \\\n",
    "    TorchDatasetProtocol\n",
    "\n",
    "# Implementation\n",
    "# ==============\n",
    "\n",
    "# Types\n",
    "# -----\n",
    "\n",
    "# This type alias refers to a function that takes in a np array and returns a np array.\n",
    "NPArrayTransform: TypeAlias = Callable[[np.ndarray], np.ndarray]\n",
    "\n",
    "# This type alias refers to a function that transforms one label into another. Note that the \n",
    "# datatype may change.\n",
    "LabelTransform: TypeAlias = Callable[[int | str], int | str]\n",
    "\n",
    "\n",
    "# Adapters\n",
    "# --------\n",
    "\n",
    "class NPArray2TorchDatasetAdapter(TorchMapDatasetProtocol, TorchIterableDatasetProtocol):\n",
    "    \"\"\"\n",
    "    This class is a helper class that allows us to convert:\n",
    "    - a dataset where each example consists of a np array and an associated \n",
    "      label in string or integer format,\n",
    "    - into a pytorch dataset.\n",
    "    It is used in the implementation of the ImageDataSetImplementation class.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            X: list[np.ndarray], \n",
    "            y: list[int | str], \n",
    "            transform: NPArrayTransform | None = None,\n",
    "            target_transform: LabelTransform | None = None,        \n",
    "            gpu: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Note that we expect input to be of type list and not iterator, because the latter does not \n",
    "        have a length method.\n",
    "        Consider making this less restrictive in the future, but it may not be necessary to support\n",
    "        iterables without length such as generators. While it is possible to require input length\n",
    "        as an argument, this gets clumsy if you want to make sure it is only required if the input \n",
    "        data structures do not have a length method (need to use generics?). \n",
    "        Even in cases where the input would just be a folder path, the validate function should be \n",
    "        able to perform the validation right after it has listed all the files. (This should even be \n",
    "        the case for a stream of batches, where the validation could take place for each batch.)\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.gpu = gpu\n",
    "        self.__validate_input()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, index) -> tuple[torch.Tensor, int | str]:\n",
    "        unlabeled_example: np.ndarray = self.X[index]\n",
    "        if self.transform is not None:\n",
    "            unlabeled_example = self.transform(unlabeled_example)\n",
    "        \n",
    "        label: int | str = self.y[index]\n",
    "        if self.target_transform is not None:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        tensor: torch.Tensor = torch.from_numpy(unlabeled_example)\n",
    "        return (tensor, label)\n",
    "    \n",
    "    def __validate_input(self) -> None | NoReturn:\n",
    "        if self.gpu == True:\n",
    "            raise NotImplementedError(\n",
    "                \"Conversion is not yet implemented when using GPU. Use Pytorch datastructures to \" \\\n",
    "                \"leverage GPU.\"\n",
    "            )\n",
    "        \n",
    "        if len(self.y) == 0:\n",
    "            raise ValueError(\"y must not be empty\")\n",
    "        \n",
    "        if len(self.X) != len(self.y):\n",
    "            raise ValueError(\"X and y must have the same length\")\n",
    "        \n",
    "        # If validation passed, we don't return anything\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def numpy(self) -> np.ndarray:\n",
    "        return np.array(self.X)\n",
    "\n",
    "\n",
    "class TorchDataset2NPArrayAdapter():\n",
    "    \"\"\"\n",
    "    This adapter takes a torch dataset and implements the same methods, but instead of tensors it \n",
    "    returns np arrays. \n",
    "    (Note that NumPy does not have an equivalent data structure to Pytorch's dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset: TorchDatasetProtocol: TypeAlias = TorchMapDatasetProtocol | TorchIterableDatasetType: TypeAlias = TorchMapDatasetProtocol | TorchIterableDatasetType: TypeAlias = TorchMapDatasetProtocol | TorchIterableDatasetType: TypeAlias = TorchMapDatasetProtocol | TorchIterableDatasetType, gpu: bool = False) -> None:\n",
    "        self.dataset = dataset\n",
    "        self.gpu = gpu\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)  # type: ignore\n",
    "    \n",
    "    def __getitem__(self, index) -> tuple[np.ndarray, int | str]:\n",
    "        if self.gpu == True:\n",
    "            raise NotImplementedError(\n",
    "                \"Conversion is not yet implemented when using GPU. Use Pytorch datastructures to\" \\\n",
    "                \"leverage GPU.\"\n",
    "            )\n",
    "        else:\n",
    "            tensor, label = self.dataset[index]\n",
    "            # Need to detach and specify device. See https://discuss.pytorch.org/t/should-it-really-be-necessary-to-do-var-detach-cpu-numpy/35489/2\n",
    "            numpy_data: np.ndarray = tensor.detach().cpu().numpy()\n",
    "            return (numpy_data, label)\n",
    "\n",
    "# class LabelFromFolderDataset(TorchDatasetProtocol: TypeAlias = TorchMapDatasetProtocol | TorchIterableDatasetType: TypeAlias = TorchMapDatasetProtocol | TorchIterableDatasetType: TypeAlias = TorchMapDatasetProtocol | TorchIterableDatasetType: TypeAlias = TorchMapDatasetProtocol | TorchIterableDatasetType):\n",
    "\n",
    "# Main implementation\n",
    "# -------------------\n",
    "\n",
    "class ImageDataSetImplementation(DataSetProtocol):\n",
    "    \"\"\"\n",
    "    This uses the pytorch format internally, but allows instantiation from and \n",
    "    conversion to numpy ~~and tensoflow~~ format as well.\n",
    "    \"\"\"\n",
    "    def __init__(self, torch_dataset: TorchDatasetProtocol: TypeAlias = TorchMapDatasetProtocol | TorchIterableDatasetType: TypeAlias = TorchMapDatasetProtocol | TorchIterableDatasetType: TypeAlias = TorchMapDatasetProtocol | TorchIterableDatasetType: TypeAlias = TorchMapDatasetProtocol | TorchIterableDatasetType, gpu: bool = False):\n",
    "        if gpu:\n",
    "            raise NotImplementedError(\"GPU support is not yet implemented\")\n",
    "        self.data = torch_dataset\n",
    "        self.gpu = gpu\n",
    "    \n",
    "\n",
    "    @classmethod\n",
    "    def from_torch(cls, torch_dataset):\n",
    "        return cls(torch_dataset=torch_dataset)\n",
    "\n",
    "    def to_torch(self) -> TorchDatasetProtocol: TypeAlias = TorchMapDatasetProtocol | TorchIterableDatasetType: TypeAlias = TorchMapDatasetProtocol | TorchIterableDatasetType: TypeAlias = TorchMapDatasetProtocol | TorchIterableDatasetType: TypeAlias = TorchMapDatasetProtocol | TorchIterableDatasetType:\n",
    "        return self.data\n",
    "    \n",
    "\n",
    "    @classmethod\n",
    "    def from_numpy(cls, X: list[np.ndarray], y: list[int | str], gpu: bool = False):\n",
    "        torch_dataset: TorchDatasetProtocol: TypeAlias = TorchMapDatasetProtocol | TorchIterableDatasetType: TypeAlias = TorchMapDatasetProtocol | TorchIterableDatasetType: TypeAlias = TorchMapDatasetProtocol | TorchIterableDatasetType: TypeAlias = TorchMapDatasetProtocol | TorchIterableDatasetType = NPArray2TorchDatasetAdapter(X=X, y=y)\n",
    "        return cls(torch_dataset=torch_dataset)\n",
    "\n",
    "    def to_numpy_dataset(self):\n",
    "        \"\"\"\n",
    "        This returns a \"numpy dataset\", i.e. an object similar to Pytorch's datasets, except that it\n",
    "        returns np arrays instead of tensors.\n",
    "        \"\"\"\n",
    "        return TorchDataset2NPArrayAdapter(dataset=self.data, gpu=self.gpu)\n",
    "\n",
    "\n",
    "    # TODO: distinguish between instantiating from tensor versus tf.dataset\n",
    "    # @classmethod\n",
    "    # def from_tensorflow(cls, data: tf.Tensor, gpu: bool = False):\n",
    "    #     if gpu == True:\n",
    "    #         raise NotImplementedError(\"Conversion is not yet implemented when using GPU. Use Pytorch datastructures to leverage GPU.\")\n",
    "    #     else:\n",
    "    #         numpy_data: np.ndarray = data.numpy()\n",
    "    #         torch_data: torch.Tensor = torch.from_numpy(numpy_data)\n",
    "    #         return cls(torch_data=torch_data)\n",
    "        \n",
    "    # def to_tensorflow(self) -> tf.Tensor:\n",
    "    #     if self.gpu == True:\n",
    "    #         raise NotImplementedError(\"Conversion is not yet implemented when using GPU. Use Pytorch datastructures to leverage GPU.\")\n",
    "    #     else:\n",
    "    #         numpy_data: np.ndarray = self.to_numpy()\n",
    "    #         tf_data: tf.Tensor = tf.convert_to_tensor(numpy_data)\n",
    "    #         return tf_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested Folders: Fungi Classification\n",
    "https://archive.ics.uci.edu/dataset/773/defungi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes data has been downloaded to this directory\n",
    "DATA_DIR_FUNGI = 'data/defungi/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from functools import cached_property\n",
    "from dataclasses import dataclass\n",
    "\n",
    "Label: TypeAlias = str | int\n",
    "\n",
    "# class _ReadFunctionConfig(ABC):\n",
    "#     \"\"\"\n",
    "#     This serves as the supertype for configs passed to the read function.  These will usually be \n",
    "#     data classes (which already have the `asdict()` method implemented), and need to be registered \n",
    "#     as a permissible type of ReadFunctionConfig by calling `ReadFunctionConfig.register(my_config)`. \n",
    "#     \"\"\"\n",
    "#     @abstractmethod\n",
    "#     def asdict(self) -> dict[str, Any]: \n",
    "#         pass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FileReader:\n",
    "    reader_function: Callable[[Any], torch.Tensor]\n",
    "    reader_function_config: dict # _ReadFunctionConfig\n",
    "\n",
    "\n",
    "# class FileReader(Protocol):\n",
    "#     @property\n",
    "#     def reader_function(self) -> Callable[[Path, _ReadFunctionConfig], torch.Tensor]:\n",
    "#         ...\n",
    "#     @property\n",
    "#     def reader_function_config(self) -> _ReadFunctionConfig:\n",
    "#         ...\n",
    "\n",
    "\n",
    "class LabelFromFoldernameDataset(TorchMapDatasetProtocol, TorchIterableDatasetProtocol):\n",
    "    \"\"\"\n",
    "    This class implements the protocols for a PyTorch Datasets (both map and list). \n",
    "    \n",
    "    (Note that the inheritance from these protocols does not make this class a Protocol itself. For \n",
    "    that it would have to inherit directly from Protocol as well.)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        file_reader: FileReader,\n",
    "        data_dir: str, \n",
    "        expected_filetype: str | None = None,\n",
    "        input_transform: Callable | None = None, \n",
    "        target_transform: Callable | None = None,\n",
    "    ):\n",
    "        self.file_reader = file_reader\n",
    "        self.data_dir: Path = Path(data_dir)\n",
    "        self.expected_filetype = expected_filetype\n",
    "        self.input_transform = input_transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    @cached_property\n",
    "    def _inputfile_to_label_mapping(self) -> dict[str, Label]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        expected_filetype: If specified, ValueError is raised unless all files in subfolder have the \n",
    "            specified extension. Do NOT include the dot in the extension. Set to None to skip check.\n",
    "\n",
    "        This function lists the paths of all input files, and maps them to the associated label, \n",
    "        which is based on the subfolder a file lives in. The reason to do this ahead of time, rather\n",
    "        than iterating over all the subfolders when we need to retrieve the data, is allows us to \n",
    "        easily shuffle the data. \n",
    "\n",
    "        Note that this approach has the downside of needing to hold the paths to all input files in \n",
    "        memory. Once this becomes a problem, we can refine this algorithm, but it will probably add \n",
    "        a lot of complexity to do so. \n",
    "        One possible solution would be to still list the number of files for each subfolder, but to \n",
    "        only store the *number* of files in each directory. When we want to retrieve the data, we \n",
    "        can then sample from the distribution of folder names/label plus index (ranging from one to \n",
    "        the number of files for that specific label).\n",
    "\n",
    "        Note on type hints: While it would be preferable to to distinguish directory from file \n",
    "        paths, I didn't find subtypes or Protocols of Path to distinguish between these. \n",
    "        ToDo: Define Protocols for Path and DirectoryPath, and use them here. \n",
    "        \"\"\"\n",
    "        def _check_filetype(file: Path, expected_filetype: str | None) -> None | NoReturn:\n",
    "            if expected_filetype is None:\n",
    "                return None  # Skip check\n",
    "\n",
    "            else:\n",
    "                # First remove dot from file suffix\n",
    "                actual_filetype =  file.suffix[1:]\n",
    "                if actual_filetype == expected_filetype:\n",
    "                    return None  # passed validation\n",
    "                else:\n",
    "                    raise ValueError(f\"{file} does not have expected extension {expected_filetype}\")\n",
    "\n",
    "        inputfile_to_label_mapping: dict[str, Label] = {}\n",
    "        subfolders: list[Path] = [\n",
    "            obj for obj in self.data_dir.iterdir() \n",
    "            if obj.is_dir()\n",
    "        ]\n",
    "\n",
    "        for subfolder in subfolders:\n",
    "            # Label is same as folder name\n",
    "            label: Label = subfolder.name  \n",
    "            # Find all files in the subfolder and save their names\n",
    "            for file in subfolder.iterdir():\n",
    "                _check_filetype(file=file, expected_filetype=self.expected_filetype)\n",
    "                inputfile_to_label_mapping[file.name] = label\n",
    "\n",
    "        return inputfile_to_label_mapping\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        This function returns the number of files in the data directory.\n",
    "        \"\"\"\n",
    "        return len(\n",
    "            self._inputfile_to_label_mapping\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index) -> tuple[torch.Tensor, int | Label]:\n",
    "        \"\"\"\n",
    "        This function returns an example (file + label) at the specified index.\n",
    "\n",
    "        Todo: Make  sure that the repeated casting to list is not taking too much time. Otherwise,\n",
    "        consider storing data in a two dimensional array, where each row contains path and label. \n",
    "        The trade-off is that this will consume more memory, though. A further optimization that \n",
    "        would avoid this memory overhead is to only store the order of labels, along with the\n",
    "        number of files for each label, as already hinted above. If the list of paths is stored in\n",
    "        the same order as the labels, it is easy to compute the label for a path at a given index.\n",
    "\n",
    "        Another alternative: Create a more memory efficient version that only implements the\n",
    "        Protocol for Torch*Iterable*DataSet.\n",
    "\n",
    "        \"\"\"\n",
    "        # todo: try dict.items()\n",
    "\n",
    "        # Get all keys, which represent the filenames. Then convert to list, so we can use use index\n",
    "        inputfiles: list[str] = list(\n",
    "            self._inputfile_to_label_mapping \\\n",
    "            .keys()\n",
    "        )\n",
    "        # Get path and label for that index\n",
    "        input_file: str = inputfiles[index]\n",
    "        label: Label = self._inputfile_to_label_mapping[input_file]\n",
    "        \n",
    "        # Load file at selected path into memory\n",
    "        absolute_path_to_input: Path = self.data_dir / str(label) / input_file\n",
    "        reader_args: dict[str, Any] = self.file_reader.reader_function_config  #.asdict()\n",
    "        input: torch.Tensor = self.file_reader.reader_function(\n",
    "            str(absolute_path_to_input), \n",
    "            **reader_args,\n",
    "        )\n",
    "        \n",
    "        # Apply transformations, if specified\n",
    "        if self.input_transform is not None:\n",
    "            input = self.input_transform(input)\n",
    "        if self.target_transform is not None:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return (input, label)\n",
    "\n",
    "\n",
    "    # def __iter__(self) -> tuple[torch.Tensor, int | Label]:\n",
    "    #     \"\"\"\n",
    "    #     This function iterates over all files in the data directory, and returns the associated \n",
    "    #     label and feature. \n",
    "    #     \"\"\"\n",
    "    #     datapath_to_label_mapping = self._inputfile_to_label_mapping\n",
    "    #     for datapath, label in datapath_to_label_mapping.items():\n",
    "    #         feature: torch.Tensor = torch.load(datapath)\n",
    "    #         if self.feature_transform is not None:\n",
    "    #             feature = self.feature_transform(feature)\n",
    "    #         if self.target_transform is not None:\n",
    "    #             label = self.target_transform(label)\n",
    "    #         yield (feature, label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<cell>3: \u001b[1m\u001b[31merror:\u001b[m Cannot instantiate abstract class\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "fungi_dataset = LabelFromFoldernameDataset(\n",
    "    file_reader=FileReader(\n",
    "        reader_function=torchvision.io.read_image,\n",
    "        reader_function_config={'mode': torchvision.io.ImageReadMode.UNCHANGED},\n",
    "    ),\n",
    "    data_dir=DATA_DIR_FUNGI,\n",
    "    expected_filetype='jpg',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[116, 117, 117,  ..., 143, 142, 142],\n",
       "          [117, 117, 118,  ..., 143, 143, 142],\n",
       "          [119, 119, 119,  ..., 144, 143, 143],\n",
       "          ...,\n",
       "          [146, 145, 144,  ..., 144, 145, 146],\n",
       "          [146, 146, 144,  ..., 144, 145, 146],\n",
       "          [147, 146, 145,  ..., 144, 145, 146]],\n",
       " \n",
       "         [[121, 122, 122,  ..., 146, 145, 145],\n",
       "          [122, 122, 123,  ..., 146, 146, 145],\n",
       "          [121, 121, 121,  ..., 147, 146, 146],\n",
       "          ...,\n",
       "          [136, 135, 134,  ..., 151, 152, 153],\n",
       "          [136, 136, 134,  ..., 151, 152, 153],\n",
       "          [137, 136, 135,  ..., 151, 152, 153]],\n",
       " \n",
       "         [[117, 118, 118,  ..., 139, 138, 138],\n",
       "          [118, 118, 119,  ..., 139, 139, 138],\n",
       "          [118, 118, 118,  ..., 140, 139, 139],\n",
       "          ...,\n",
       "          [137, 136, 135,  ..., 144, 145, 146],\n",
       "          [137, 137, 135,  ..., 144, 145, 146],\n",
       "          [138, 137, 136,  ..., 144, 145, 146]]], dtype=torch.uint8),\n",
       " 'H1')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fungi_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9114"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fungi_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oject-oriented-ml-2RS15okd-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
