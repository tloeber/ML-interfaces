{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note on license:\n",
    "This notebook uses examples from the official Lightning repo, which is licensed under Apache 2.0. In compliance with the Apache license, any reused code is relicensed under the license in this project (as of September 2023, the MIT license), but I list modifications to the original code here:\n",
    "- Refactor PyTorch Lightning example code so it can be used with an adapter class from this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't use strict note for type checking yet\n",
    "%nb_mypy mypy-options --pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod, abstractproperty, ABC\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "\n",
    "from utils.why_not_generic_data_containers import DataSetInterface\n",
    "\n",
    "# Data Set\n",
    "# ========\n",
    "\n",
    "class ImageDataSetImplementation(DataSetInterface):\n",
    "    \"\"\"\n",
    "    In this simple example, we store the data as a TorchDataset, so we don't \n",
    "    have to do any transformation upon instantiation or retrieval.\n",
    "    \"\"\"\n",
    "    def __init__(self, data: TorchDataset):\n",
    "        self.data = data\n",
    "    \n",
    "    @classmethod\n",
    "    def from_torch(cls, data):\n",
    "        return cls(data=data)\n",
    "\n",
    "    def to_torch(self) -> TorchDataset:\n",
    "        return self.data\n",
    "\n",
    "\n",
    "# Container\n",
    "# =========\n",
    "\n",
    "class DataContainer():\n",
    "    \"\"\"\n",
    "    This class is using the simplest possible way to define a container: It \n",
    "    simply has a property for each of the three subsets. Note that we are not \n",
    "    implementing generics yet to distinguish containers for different kinds of \n",
    "    data sets.\n",
    "    (Note also that it does not have a method implemented to retrieve the \n",
    "    complete set, because this is not necessary for our current testing \n",
    "    purposes.) \n",
    "    \"\"\"\n",
    "    def __init__(self, train: DataSetInterface, val: DataSetInterface, test: DataSetInterface):\n",
    "        self._train = train\n",
    "        self._val = val\n",
    "        self._test = test\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        return self._train\n",
    "    \n",
    "    @property\n",
    "    def val(self):\n",
    "        return self._val\n",
    "    \n",
    "    @property\n",
    "    def test(self):\n",
    "        return self._test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we run into problems with our simple design of making the container simply an aggregation of the different subsets: Since this makes the data sets the place where we store logic to return the data in a specified format, this does not work out-of-the-box with the concept of **data loaders** in libraries such as Pytorch: *Data loaders are classes similar to our data containers* in the sense that they allow retrieving different subsets; however, they differ in the fact that they are specific for particular ML frameworks, and thus expect the data sets to be returned in the model specific framework when we call a specifically named method. The problem is that it is not enough if our data container can *return* the data in the required format, but it would also need to follow the specific *interface* of the data loader. For example, it would need to return the training data when the train_dataloader() method is called, whereas we would achieve this with our data container by calling something like get_validation_set().to_torch().\n",
    "\n",
    "Of course, this problem is not insurmountable, because we can simply leverage the *adapter* design pattern to translate one interface into another. This could look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader as TorchDataLoader\n",
    "from lightning.pytorch import LightningDataModule, LightningModule\n",
    "\n",
    "class LightningDataloaderAdapter(LightningDataModule):\n",
    "    \"\"\"\n",
    "    This adapter class takes an instance of our data container and converts it \n",
    "    into a LightningDataModule.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_container, batch_size) -> None:\n",
    "        super().__init__()\n",
    "        self.data_container = data_container\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return TorchDataLoader(\n",
    "            dataset=self.data_container.train.to_torch(),\n",
    "            batch_size=self.batch_size\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return TorchDataLoader(\n",
    "            dataset=self.data_container.val.to_torch(), \n",
    "            batch_size=self.batch_size\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return TorchDataLoader(\n",
    "            dataset=self.data_container.test.to_torch(), \n",
    "            batch_size=self.batch_size\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self, data):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data loader adapter can then be used by our OO_ML estimators, as shown in below cell: Our estimator will get the data is a data container, and can then use this adapter in order to convert the data container into a data loader. The key line is this in the \\_\\_init\\_\\_():\n",
    "```\n",
    "self.data_loader = LightningDataloaderAdapter(data_container=data_container, ...)        \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "import lightning as L\n",
    "\n",
    "from utils.why_not_generic_data_containers import ModelConfig, EstimatorInterface\n",
    "\n",
    "class PytorchLightningAdapter(EstimatorInterface):\n",
    "    \"\"\"\n",
    "    This adaptor class takes a PyTorch Lightning classifier and converts its \n",
    "    interface to our OO_ML estimator interface.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        Classifier: type[LightningModule],\n",
    "        data_container: DataContainer,\n",
    "        config: ModelConfig,\n",
    "    ) -> None:\n",
    "        self.config = config\n",
    "        self.classifier = Classifier()\n",
    "        \n",
    "        # Convert data container to lightning data loader\n",
    "        # ***********************************************\n",
    "        self.data_loader =  LightningDataloaderAdapter(\n",
    "            data_container=data_container,\n",
    "            batch_size=config.batch_size,    \n",
    "        )\n",
    "        # ***********************************************\n",
    "    \n",
    "    def fit(self):\n",
    "        trainer = L.Trainer(fast_dev_run=config.fast_dev_run)\n",
    "        trainer.fit(\n",
    "            model=self.classifier,\n",
    "            datamodule=self.data_loader,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, while it is good to have the option of leveraging the adapter pattern, it is better to avoid this is possible. It turns out that this is relatively easy to avoid by simply making the data container (rather than the data set) the place that returns data in specific formats. This allows us to simply give the data container a to_dataloader() method, making the logic more straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# First we need to define the data loader class that we want to return. It will \n",
    "# contain a constructor that will allow instantiating it from a data container.\n",
    "class LightningDataLoader(LightningDataModule):\n",
    "    def __init__(self, train_data, val_data, test_data, transforms, batch_size) -> None:\n",
    "        super().__init__()\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.test_data = test_data\n",
    "        self.batch_size = batch_size\n",
    "        self.transforms = transforms        \n",
    "\n",
    "    # This is the constructor we will use\n",
    "    @classmethod\n",
    "    def from_data_container(cls, data_container, batch_size: int, transforms):\n",
    "        return cls(\n",
    "            train_data=data_container.train.to_torch(),\n",
    "            val_data=data_container.val.to_torch(),\n",
    "            test_data=data_container.test.to_torch(),\n",
    "            batch_size=batch_size,\n",
    "            transforms=transforms,\n",
    "        )\n",
    "\n",
    "    def setup(self):\n",
    "        # Todo: Split data here\n",
    "        pass \n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return TorchDataLoader(\n",
    "            dataset=self.train_data,\n",
    "            batch_size=self.batch_size\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return TorchDataLoader(\n",
    "            dataset=self.val_data, \n",
    "            batch_size=self.batch_size\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return TorchDataLoader(\n",
    "            dataset=self.test_data, \n",
    "            batch_size=self.batch_size\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self, data):\n",
    "        pass\n",
    "\n",
    "\n",
    "class DataContainer_v2():\n",
    "    def __init__(self, train: DataSetInterface, val: DataSetInterface, test: DataSetInterface):\n",
    "        self._train = train\n",
    "        self._val = val\n",
    "        self._test = test\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        return self._train\n",
    "    \n",
    "    @property\n",
    "    def val(self):\n",
    "        return self._val\n",
    "    \n",
    "    @property\n",
    "    def test(self):\n",
    "        return self._test\n",
    "    \n",
    "    def to_lightning_dataloader(self, batch_size) -> LightningDataLoader:\n",
    "        return LightningDataLoader.from_data_container(\n",
    "            data_container=self,\n",
    "            batch_size=batch_size,\n",
    "            transforms=None\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "import lightning as L\n",
    "\n",
    "from utils.why_not_generic_data_containers import ModelConfig, EstimatorInterface\n",
    "\n",
    "class PytorchLightningAdapter_v2(EstimatorInterface):\n",
    "    \"\"\"\n",
    "    This adaptor class takes a PyTorch Lightning classifier and converts its \n",
    "    interface to our OO_ML estimator interface.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        Classifier: type[LightningModule],\n",
    "        data_container: DataContainer_v2,  # Note we are using the new container\n",
    "        config: ModelConfig,\n",
    "    ) -> None:\n",
    "        self.config = config\n",
    "        self.classifier = Classifier()\n",
    "\n",
    "        # Convert data container to lightning data loader\n",
    "        # ************************************************\n",
    "        self.data_loader = data_container.to_lightning_dataloader(\n",
    "            batch_size=config.batch_size,    \n",
    "        )\n",
    "        # ************************************************\n",
    "        \n",
    "    \n",
    "    def fit(self):\n",
    "        trainer = L.Trainer(fast_dev_run=config.fast_dev_run)\n",
    "        trainer.fit(\n",
    "            model=self.classifier,\n",
    "            datamodule=self.data_loader,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Either way works, but I think that adding that to_lightning_dataloadermethod to the data container is slightly preferable because:\n",
    "- It is less complex because it requires using only one rather than two adapters. (However, since the data loader adapter would be used in the lightning adapter, most of the added complexity is at least hidden from the library's user.)\n",
    "- It may lead to better code organization, because the code for constructing the lightning adapter is located near the data container code.\n",
    "\n",
    "However, I'm still somewhat on the fence because of the following disadvantages of this solution:\n",
    "- The data container now needs to know about the data set's internals. We need to be careful this doesn't lead to any problems, such as circular imports.\n",
    "- Except for PyTorch lightning, we have separate classes for training, validation, and test sets, so the conversion to the data format required by a specific library (e.g., to_numpy_array) is most naturally handled at the data *set* level, not the data *container* level. This raises the question whether we want to move this data format conversion from the data set to the container level for all ML libraries, or whether we are okay with this slight inconsistency of only handling it at the container level for Lightning. At this point I'm not ready to make a decision yet, but will experiment with the implications of both choices.\n",
    "\n",
    "In any case, it is not ideal that we are letting a single library (Pytorch Lightning) determine our overall design. However, in the end I still think this is the right decision because:\n",
    "- Lightning is one of the best libraries for many deep learning use cases;\n",
    "- [Even when using plain PyTorch, you can still use Lightning's data module](https://pytorch-lightning.readthedocs.io/en/0.9.0/datamodules.html#datamodules-without-lightning) for better encapsulation;\n",
    "- it is possible that we will encounter other libraries using this same design for the data loader;\n",
    "- it is possible that libraries that do not currently raise this problem may add the same ability for a higher-level data loader as Lightning.\n",
    "\n",
    "# Appendix: Model Run\n",
    "The code below is not relevant to the main point of this notebook, but it is useful for verifying that everything is working. Note, however, that the **way of interacting with the model stays the same, independent of which container implementation we use.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example data\n",
    "# ===================\n",
    "\n",
    "from datetime import datetime \n",
    "\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from utils.why_not_generic_data_containers import LitClassifier\n",
    "\n",
    "DATA_DIR = './data'\n",
    "\n",
    "mnist_train_and_val = MNIST(\n",
    "    root=DATA_DIR, \n",
    "    train=True, \n",
    "    download=True,\n",
    "        transform=transforms.ToTensor()\n",
    ")\n",
    "mnist_train, mnist_val = random_split(\n",
    "    dataset=mnist_train_and_val, \n",
    "    lengths=[.9, .1]\n",
    ")\n",
    "mnist_test = MNIST(\n",
    "    root=DATA_DIR, \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "mnist_train = ImageDataSetImplementation.from_torch(mnist_train)\n",
    "mnist_val = ImageDataSetImplementation.from_torch(mnist_val)\n",
    "mnist_test = ImageDataSetImplementation.from_torch(mnist_test)\n",
    "\n",
    "# v1 of container (requires data loader adapter)\n",
    "mnist_container = DataContainer(\n",
    "    train=mnist_train,\n",
    "    val=mnist_val,\n",
    "    test=mnist_test\n",
    ")\n",
    "\n",
    "# v2 of container with to_lightning_dataloader() method\n",
    "mnist_container_v2 = DataContainer_v2(\n",
    "    train=mnist_train,\n",
    "    val=mnist_val,\n",
    "    test=mnist_test\n",
    ")\n",
    "\n",
    "\n",
    "config = ModelConfig(\n",
    "    batch_size=64,\n",
    "    fast_dev_run=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 1: Using adapter for data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n",
      "\n",
      "  | Name     | Type     | Params\n",
      "--------------------------------------\n",
      "0 | backbone | Backbone | 101 K \n",
      "--------------------------------------\n",
      "101 K     Trainable params\n",
      "0         Non-trainable params\n",
      "101 K     Total params\n",
      "0.407     Total estimated model params size (MB)\n",
      "/home/thomas/.cache/pypoetry/virtualenvs/oject-oriented-ml-2RS15okd-py3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/thomas/.cache/pypoetry/virtualenvs/oject-oriented-ml-2RS15okd-py3.11/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "/home/thomas/.cache/pypoetry/virtualenvs/oject-oriented-ml-2RS15okd-py3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008dc09999034efba49b01a3d1a26bfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0690abea2d941fbabee3900c87e5fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = PytorchLightningAdapter(\n",
    "        Classifier=LitClassifier,\n",
    "        data_container=mnist_container,\n",
    "        config=config,\n",
    "    )\n",
    "    model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 2: Giving container a to_data_loader method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n",
      "\n",
      "  | Name     | Type     | Params\n",
      "--------------------------------------\n",
      "0 | backbone | Backbone | 101 K \n",
      "--------------------------------------\n",
      "101 K     Trainable params\n",
      "0         Non-trainable params\n",
      "101 K     Total params\n",
      "0.407     Total estimated model params size (MB)\n",
      "/home/thomas/.cache/pypoetry/virtualenvs/oject-oriented-ml-2RS15okd-py3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/thomas/.cache/pypoetry/virtualenvs/oject-oriented-ml-2RS15okd-py3.11/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "/home/thomas/.cache/pypoetry/virtualenvs/oject-oriented-ml-2RS15okd-py3.11/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709e279c8e9d4e368a920741e51abfb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "454db611fc384294bebf64ea66422165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model_v2 = PytorchLightningAdapter_v2(\n",
    "        Classifier=LitClassifier,\n",
    "        data_container=mnist_container_v2,\n",
    "        config=config,\n",
    "    )\n",
    "    model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-14 09:23:04.533447\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oject-oriented-ml-2RS15okd-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
